{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Softmax and cross entropy \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x,dim):\n",
    "    return np.exp(x) / np.sum(np.exp(x),axis= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy : [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([2.0,1.0,0.1])\n",
    "ouput = softmax(y)\n",
    "print(\"softmax numpy :\" ,ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax torch  tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "##tensor \n",
    "x = torch.tensor([2.0,1.0,0.1])\n",
    "ouput = torch.softmax(x,dim=0)\n",
    "print(\"softmax torch \",ouput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###cross entropy in numpy \n",
    "def cross_entropy (actual,predicted):\n",
    "    loss =- np.sum(actual* np.log(predicted))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss numpy :0.356675\n",
      "Loss numpy 2.302585\n"
     ]
    }
   ],
   "source": [
    "##our y should be onehotencoded \n",
    "Y = np.array([1,0,0])\n",
    "##y_pred propabilities \n",
    "y_pred_good = np.array([0.7,0.2,0.1])\n",
    "y_pred_bad = np.array([0.1,0.3,0.6])\n",
    "\n",
    "loss1 = cross_entropy(Y,y_pred_good)\n",
    "loss2 = cross_entropy(Y,y_pred_bad)\n",
    "\n",
    "print(f\"Loss numpy :{loss1:4f}\")\n",
    "print(f\"Loss numpy {loss2:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss torch :0.35667494393873245\n",
      "Loss torch 2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "## cross entropy in pytorch \n",
    "## before we apply cross entropy loss in pytorch we have to know we don't need that our Y to be onehotencoded also y_prediction has row scores no softmax at all \n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "Y = torch.tensor([0])\n",
    "\n",
    "##nsamples * nclasses =1 X3 \n",
    "y_pred_good_t = torch.tensor([[2.0,1.0,0.1]])\n",
    " \n",
    "y_pred_bad_t= torch.tensor([[0.5,2.0,0.3]])\n",
    "ls1 = loss(y_pred_good_t,Y)\n",
    "ls2 = loss(y_pred_bad_t,Y)\n",
    "print(f\"Loss torch :{loss1.item()}\")\n",
    "print(f\"Loss torch {loss2.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Multiclass problem \n",
    " ##num_classes = ouput \n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_classes):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet1(input_size =28*28,hidden_size=5,num_classes=3)\n",
    "##loss \n",
    "criteria = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions \n",
    "\n",
    " * Most popular activatoin functions \n",
    "* Step function , sigmoid , tANh , Relu ,LeakyRelu ,Softmax\n",
    "\n",
    "* In sigmoid we use last layer of binary classification problem\n",
    "* Tanh function it will output value between -1 or 1 \n",
    "*  Relu funtion : it will output zero for negative values and it's non linear in some cases if you don't know what activation funciton to use just use Relu \n",
    "*  Leaky relu function :improved version of Relu \n",
    "* Softmax funtion : basically it will just squash the input to be outputs between 0 and 1  , the difference between sigmoid and softmax , in sigmoid we use for two classic logistic regression while in softmax we use for multiclass logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Multiclass problem \n",
    " ##num_classes = ouput \n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_classes):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        ### available activation function \n",
    "        nn.ReLU()\n",
    "        nn.Sigmoid()\n",
    "        nn.Softmax()\n",
    "        nn.Tanh()\n",
    "        nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "336d0fd160d94e0e0f6591eaf10611da6cd73f7da6c7ea5a76062720089abcab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
